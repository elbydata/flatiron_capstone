{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4: Batch Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the construction and training of the model iterations and various experiments. The notebook is split up into four sections: training mode selection (where the model will run), set-up, model constrution, and training. \n",
    "\n",
    "Evaluation will take place in the *model_optimization_and_evaluation.ipynb* notebook found in the *notebooks* folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# importing necessary packages and libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras import applications \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Training Mode Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, specify the training mode for the model. This will determine the location from which the source data is drawn, and to which the trained models (and training histories) are saved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **training_mode = 'floydhub'** (runs on Floydhub)\n",
    "- **training_mode = 'local'** (runs on local disk and processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select training mode\n",
    "training_mode = 'floydhub'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory base paths\n",
    "data_path_local = '../../data/0002_array_data/train_data/'\n",
    "model_path_local = '../../notebooks/model_construction/saved_models/'\n",
    "data_path_floydhub = '/floyd/input/capstone_mushrooms/'\n",
    "model_path_floydhub = '/floyd/home/'\n",
    "\n",
    "# setting directory paths based on training mode selection\n",
    "if training_mode == 'floydhub':\n",
    "    data_path = data_path_floydhub\n",
    "    model_path = model_path_floydhub\n",
    "elif training_mode == 'local':\n",
    "    data_path = data_path_local\n",
    "    model_path = model_path_local\n",
    "else:\n",
    "    raise Exception('Please choose valid training mode: \"floydhub\" or \"local\".')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the training and validation data subsets\n",
    "X_train = np.load(f'{data_path}X_train_data.npy')\n",
    "y_train = np.load(f'{data_path}y_train_data.npy')\n",
    "X_val = np.load(f'{data_path}X_val_data.npy')\n",
    "y_val = np.load(f'{data_path}y_val_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting training parameters\n",
    "# batch_size = 32\n",
    "n_classes = 20\n",
    "n_epochs = 15\n",
    "img_shape = X_train.shape[1:]\n",
    "model_names = []\n",
    "model_list = []\n",
    "model_hists = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Model Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial 1 - 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 200, 200, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 100, 100, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 100, 100, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 50, 50, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 50, 50, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 25, 25, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 25, 25, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 36864)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               18874880  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                10260     \n",
      "=================================================================\n",
      "Total params: 19,273,556\n",
      "Trainable params: 19,273,556\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# defining the model architecture\n",
    "m4_t1 = models.Sequential()\n",
    "\n",
    "# convolution/max pool stacks\n",
    "m4_t1.add(layers.Conv2D(32,(3,3), activation='relu', input_shape=img_shape, padding='same'))\n",
    "m4_t1.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "m4_t1.add(layers.Conv2D(64,(3,3), activation='relu', padding='same'))\n",
    "m4_t1.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "m4_t1.add(layers.Conv2D(128,(3,3), activation='relu', padding='same'))\n",
    "m4_t1.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "m4_t1.add(layers.Conv2D(256,(3,3), activation='relu', padding='same'))\n",
    "m4_t1.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "# fully connected layers\n",
    "m4_t1.add(layers.Flatten())\n",
    "m4_t1.add(layers.Dense(512, activation='relu'))\n",
    "m4_t1.add(layers.Dense(n_classes, activation='softmax'))\n",
    "\n",
    "# reviewing the model architecture and adding model and name to list\n",
    "m4_t1.summary()\n",
    "model_names.append('m4_t1')\n",
    "model_list.append(m4_t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial 2 - 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 200, 200, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 100, 100, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 100, 100, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 50, 50, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 50, 50, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 25, 25, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 25, 25, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 36864)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               18874880  \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 20)                10260     \n",
      "=================================================================\n",
      "Total params: 19,273,556\n",
      "Trainable params: 19,273,556\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# defining the model architecture\n",
    "m4_t2 = models.Sequential()\n",
    "\n",
    "# convolution/max pool stacks\n",
    "m4_t2.add(layers.Conv2D(32,(3,3), activation='relu', input_shape=img_shape, padding='same'))\n",
    "m4_t2.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "m4_t2.add(layers.Conv2D(64,(3,3), activation='relu', padding='same'))\n",
    "m4_t2.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "m4_t2.add(layers.Conv2D(128,(3,3), activation='relu', padding='same'))\n",
    "m4_t2.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "m4_t2.add(layers.Conv2D(256,(3,3), activation='relu', padding='same'))\n",
    "m4_t2.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "# fully connected layers\n",
    "m4_t2.add(layers.Flatten())\n",
    "m4_t2.add(layers.Dense(512, activation='relu'))\n",
    "m4_t2.add(layers.Dense(n_classes, activation='softmax'))\n",
    "\n",
    "# reviewing the model architecture and adding model and name to list\n",
    "m4_t2.summary()\n",
    "model_names.append('m4_t2')\n",
    "model_list.append(m4_t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up standardization and augmentation parameters\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   rotation_range=40,\n",
    "                                   width_shift_range=0.3,\n",
    "                                   height_shift_range=0.3,\n",
    "                                   shear_range=0.3,\n",
    "                                   zoom_range=0.3,\n",
    "                                   fill_mode='nearest',\n",
    "                                   horizontal_flip=True,\n",
    "                                   vertical_flip=True)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 880 ms, sys: 2.56 s, total: 3.44 s\n",
      "Wall time: 3.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# data standardization and augmentation\n",
    "train_generator_1 = train_datagen.flow(X_train, y_train, batch_size=8)\n",
    "val_generator_1 = val_datagen.flow(X_val, y_val, batch_size=8)\n",
    "train_generator_2 = train_datagen.flow(X_train, y_train, batch_size=128)\n",
    "val_generator_2 = val_datagen.flow(X_val, y_val, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling loss functions\n",
    "m4_t1.compile(loss='categorical_crossentropy', optimizer='adamax', metrics=['acc'])\n",
    "m4_t2.compile(loss='categorical_crossentropy', optimizer='adamax', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1013/1013 [==============================] - 150s 148ms/step - loss: 2.7123 - acc: 0.1410 - val_loss: 2.6837 - val_acc: 0.2562\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.25619, saving model to /floyd/home/m4_t1.h5\n",
      "Epoch 2/15\n",
      "1013/1013 [==============================] - 81s 80ms/step - loss: 2.3319 - acc: 0.2485 - val_loss: 2.6121 - val_acc: 0.3192\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.25619 to 0.31920, saving model to /floyd/home/m4_t1.h5\n",
      "Epoch 3/15\n",
      "1013/1013 [==============================] - 81s 80ms/step - loss: 2.2204 - acc: 0.2794 - val_loss: 2.5052 - val_acc: 0.3416\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.31920 to 0.34165, saving model to /floyd/home/m4_t1.h5\n",
      "Epoch 4/15\n",
      "1013/1013 [==============================] - 81s 80ms/step - loss: 2.0865 - acc: 0.3320 - val_loss: 1.7294 - val_acc: 0.3853\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.34165 to 0.38529, saving model to /floyd/home/m4_t1.h5\n",
      "Epoch 5/15\n",
      "1013/1013 [==============================] - 82s 81ms/step - loss: 2.0017 - acc: 0.3585 - val_loss: 1.8076 - val_acc: 0.4601\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.38529 to 0.46010, saving model to /floyd/home/m4_t1.h5\n",
      "Epoch 6/15\n",
      "1013/1013 [==============================] - 81s 80ms/step - loss: 1.9093 - acc: 0.3878 - val_loss: 1.4587 - val_acc: 0.4401\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.46010\n",
      "Epoch 7/15\n",
      "1013/1013 [==============================] - 81s 80ms/step - loss: 1.8642 - acc: 0.4054 - val_loss: 2.0407 - val_acc: 0.4838\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.46010 to 0.48379, saving model to /floyd/home/m4_t1.h5\n",
      "Epoch 8/15\n",
      "1013/1013 [==============================] - 82s 81ms/step - loss: 1.8044 - acc: 0.4249 - val_loss: 2.4510 - val_acc: 0.4776\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.48379\n",
      "Epoch 9/15\n",
      "1013/1013 [==============================] - 81s 80ms/step - loss: 1.7666 - acc: 0.4304 - val_loss: 0.5147 - val_acc: 0.5212\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.48379 to 0.52120, saving model to /floyd/home/m4_t1.h5\n",
      "Epoch 10/15\n",
      "1013/1013 [==============================] - 82s 81ms/step - loss: 1.6980 - acc: 0.4548 - val_loss: 1.9372 - val_acc: 0.5100\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.52120\n",
      "Epoch 11/15\n",
      "1013/1013 [==============================] - 82s 81ms/step - loss: 1.6610 - acc: 0.4635 - val_loss: 1.0980 - val_acc: 0.5499\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.52120 to 0.54988, saving model to /floyd/home/m4_t1.h5\n",
      "Epoch 12/15\n",
      "1013/1013 [==============================] - 79s 78ms/step - loss: 1.6227 - acc: 0.4814 - val_loss: 1.3601 - val_acc: 0.5549\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.54988 to 0.55486, saving model to /floyd/home/m4_t1.h5\n",
      "Epoch 13/15\n",
      "1013/1013 [==============================] - 79s 78ms/step - loss: 1.5842 - acc: 0.4927 - val_loss: 2.1820 - val_acc: 0.5661\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.55486 to 0.56608, saving model to /floyd/home/m4_t1.h5\n",
      "Epoch 14/15\n",
      "1013/1013 [==============================] - 78s 77ms/step - loss: 1.5541 - acc: 0.5021 - val_loss: 1.6886 - val_acc: 0.5474\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.56608\n",
      "Epoch 15/15\n",
      "1013/1013 [==============================] - 78s 77ms/step - loss: 1.5247 - acc: 0.5123 - val_loss: 1.3750 - val_acc: 0.5449\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.56608\n",
      "CPU times: user 27min 48s, sys: 2min 51s, total: 30min 40s\n",
      "Wall time: 21min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# setting up model saving checkpoints\n",
    "m4_t1_cp = ModelCheckpoint(filepath=f'{model_path}m4_t1.h5',\n",
    "                              monitor='val_acc',\n",
    "                              verbose=1,\n",
    "                              save_best_only=True)\n",
    "\n",
    "# fitting model\n",
    "m4_t1_history = m4_t1.fit(train_generator_1,\n",
    "                                steps_per_epoch=len(X_train)//8,\n",
    "                                epochs=n_epochs,\n",
    "                                callbacks=[m4_t1_cp],\n",
    "                                validation_data=val_generator_1,\n",
    "                                validation_steps=len(X_val)//8)\n",
    "\n",
    "# adding training history to list\n",
    "model_hists.append(m4_t1_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "63/63 [==============================] - 75s 1s/step - loss: 2.9776 - acc: 0.0962 - val_loss: 2.7815 - val_acc: 0.1758\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.17578, saving model to /floyd/home/m4_t2.h5\n",
      "Epoch 2/15\n",
      "63/63 [==============================] - 74s 1s/step - loss: 2.6385 - acc: 0.1629 - val_loss: 2.3672 - val_acc: 0.2185\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.17578 to 0.21848, saving model to /floyd/home/m4_t2.h5\n",
      "Epoch 3/15\n",
      "63/63 [==============================] - 73s 1s/step - loss: 2.4227 - acc: 0.2247 - val_loss: 2.1696 - val_acc: 0.2845\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.21848 to 0.28446, saving model to /floyd/home/m4_t2.h5\n",
      "Epoch 4/15\n",
      "63/63 [==============================] - 73s 1s/step - loss: 2.2881 - acc: 0.2708 - val_loss: 1.9673 - val_acc: 0.3416\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.28446 to 0.34164, saving model to /floyd/home/m4_t2.h5\n",
      "Epoch 5/15\n",
      "63/63 [==============================] - 74s 1s/step - loss: 2.1891 - acc: 0.2915 - val_loss: 1.8301 - val_acc: 0.3548\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.34164 to 0.35484, saving model to /floyd/home/m4_t2.h5\n",
      "Epoch 6/15\n",
      "63/63 [==============================] - 73s 1s/step - loss: 2.1080 - acc: 0.3186 - val_loss: 1.7954 - val_acc: 0.3900\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.35484 to 0.39003, saving model to /floyd/home/m4_t2.h5\n",
      "Epoch 7/15\n",
      "63/63 [==============================] - 73s 1s/step - loss: 2.0620 - acc: 0.3382 - val_loss: 2.0208 - val_acc: 0.4106\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.39003 to 0.41056, saving model to /floyd/home/m4_t2.h5\n",
      "Epoch 8/15\n",
      "63/63 [==============================] - 74s 1s/step - loss: 2.0192 - acc: 0.3397 - val_loss: 2.0592 - val_acc: 0.4258\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.41056 to 0.42578, saving model to /floyd/home/m4_t2.h5\n",
      "Epoch 9/15\n",
      "63/63 [==============================] - 73s 1s/step - loss: 1.9846 - acc: 0.3643 - val_loss: 1.6520 - val_acc: 0.4370\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.42578 to 0.43695, saving model to /floyd/home/m4_t2.h5\n",
      "Epoch 10/15\n",
      "63/63 [==============================] - 74s 1s/step - loss: 1.9343 - acc: 0.3727 - val_loss: 1.5000 - val_acc: 0.4677\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.43695 to 0.46774, saving model to /floyd/home/m4_t2.h5\n",
      "Epoch 11/15\n",
      "63/63 [==============================] - 73s 1s/step - loss: 1.9072 - acc: 0.3873 - val_loss: 1.8513 - val_acc: 0.4003\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.46774\n",
      "Epoch 12/15\n",
      "63/63 [==============================] - 73s 1s/step - loss: 1.8727 - acc: 0.3975 - val_loss: 1.4854 - val_acc: 0.4648\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.46774\n",
      "Epoch 13/15\n",
      "63/63 [==============================] - 74s 1s/step - loss: 1.8421 - acc: 0.4075 - val_loss: 1.6803 - val_acc: 0.4384\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.46774\n",
      "Epoch 14/15\n",
      "63/63 [==============================] - 73s 1s/step - loss: 1.8272 - acc: 0.4168 - val_loss: 1.7142 - val_acc: 0.4736\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.46774 to 0.47361, saving model to /floyd/home/m4_t2.h5\n",
      "Epoch 15/15\n",
      "63/63 [==============================] - 74s 1s/step - loss: 1.7714 - acc: 0.4298 - val_loss: 1.6526 - val_acc: 0.4818\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.47361 to 0.48177, saving model to /floyd/home/m4_t2.h5\n",
      "CPU times: user 20min 7s, sys: 2min 28s, total: 22min 36s\n",
      "Wall time: 18min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# setting up model saving checkpoints\n",
    "m4_t2_cp = ModelCheckpoint(filepath=f'{model_path}m4_t2.h5',\n",
    "                              monitor='val_acc',\n",
    "                              verbose=1,\n",
    "                              save_best_only=True)\n",
    "\n",
    "# fitting model\n",
    "m4_t2_history = m4_t2.fit(train_generator_2,\n",
    "                                steps_per_epoch=len(X_train)//128,\n",
    "                                epochs=n_epochs,\n",
    "                                callbacks=[m4_t2_cp],\n",
    "                                validation_data=val_generator_2,\n",
    "                                validation_steps=len(X_val)//128)\n",
    "\n",
    "# adding training history to list\n",
    "model_hists.append(m4_t2_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dictionary for model names, models, and histories from respective lists\n",
    "models_dict = {i:[j,k] for i,j,k in zip(model_names,model_list,model_hists)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 2s 15ms/step\n",
      "m4_t1 Val Accuracy 1: 54.94%\n",
      "m4_t1 Val Loss 1: 0.1016\n",
      "---\n",
      "7/7 [==============================] - 1s 180ms/step\n",
      "m4_t1 Val Accuracy 2: 49.26%\n",
      "m4_t1 Val Loss 2: 1.2017\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "(val_loss_1, val_accuracy_1) = m4_t1.evaluate(val_generator_1,verbose=1)\n",
    "print(f'{key} Val Accuracy 1: {round((val_accuracy_1*100),2)}%')\n",
    "print(f'{key} Val Loss 1: {round(val_loss_1,4)}')\n",
    "print('---')\n",
    "(val_loss_2, val_accuracy_2) = m4_t2.evaluate(val_generator_2,verbose=1)\n",
    "print(f'{key} Val Accuracy 2: {round((val_accuracy_2*100),2)}%')\n",
    "print(f'{key} Val Loss 2: {round(val_loss_2,4)}')\n",
    "print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m4_t1_history saved in /floyd/home/\n",
      "m4_t2_history saved in /floyd/home/\n"
     ]
    }
   ],
   "source": [
    "# saving training histories\n",
    "for key, value in models_dict.items():\n",
    "    with open(f'{model_path}{key}_history', 'wb') as file_pi:\n",
    "        pickle.dump(value[1].history, file_pi)\n",
    "    print(f'{key}_history saved in {model_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
