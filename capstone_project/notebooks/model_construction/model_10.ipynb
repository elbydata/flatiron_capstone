{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 10: Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the construction and training of the model iterations and various experiments. The notebook is split up into four sections: training mode selection (where the model will run), set-up, model constrution, and training. \n",
    "\n",
    "Evaluation will take place in the *model_optimization_and_evaluation.ipynb* notebook found in the *notebooks* folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# importing necessary packages and libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras import applications \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Training Mode Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, specify the training mode for the model. This will determine the location from which the source data is drawn, and to which the trained models (and training histories) are saved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **training_mode = 'floydhub'** (runs on Floydhub)\n",
    "- **training_mode = 'local'** (runs on local disk and processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select training mode\n",
    "training_mode = 'floydhub'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory base paths\n",
    "data_path_local = '../../data/0002_array_data/train_data/'\n",
    "model_path_local = '../../notebooks/model_construction/saved_models/'\n",
    "data_path_floydhub = '/floyd/input/capstone_mushrooms/'\n",
    "model_path_floydhub = '/floyd/home/'\n",
    "\n",
    "# setting directory paths based on training mode selection\n",
    "if training_mode == 'floydhub':\n",
    "    data_path = data_path_floydhub\n",
    "    model_path = model_path_floydhub\n",
    "elif training_mode == 'local':\n",
    "    data_path = data_path_local\n",
    "    model_path = model_path_local\n",
    "else:\n",
    "    raise Exception('Please choose valid training mode: \"floydhub\" or \"local\".')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the training and validation data subsets\n",
    "X_train = np.load(f'{data_path}X_train_data.npy')\n",
    "y_train = np.load(f'{data_path}y_train_data.npy')\n",
    "X_val = np.load(f'{data_path}X_val_data.npy')\n",
    "y_val = np.load(f'{data_path}y_val_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting training parameters\n",
    "batch_size = 8\n",
    "n_classes = 20\n",
    "n_epochs = 30\n",
    "img_shape = X_train.shape[1:]\n",
    "model_names = []\n",
    "model_list = []\n",
    "model_hists = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Model Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial 1 - Batch normalization before activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 200, 200, 32)      864       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 200, 200, 32)      128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 200, 200, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 100, 100, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 100, 100, 64)      18432     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100, 100, 64)      256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 100, 100, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 50, 50, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 50, 50, 128)       73728     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 50, 50, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 50, 50, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 25, 25, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 25, 25, 256)       294912    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 25, 25, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 25, 25, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 12, 12, 512)       1179648   \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 12, 12, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 12, 12, 512)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 18432)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              18875392  \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                20500     \n",
      "=================================================================\n",
      "Total params: 20,467,444\n",
      "Trainable params: 20,465,460\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# defining the model architecture\n",
    "m10_t1 = models.Sequential()\n",
    "\n",
    "# convolution/max pool stacks\n",
    "m10_t1.add(layers.Conv2D(32,(3,3), input_shape=img_shape, padding='same', use_bias=False))\n",
    "m10_t1.add(layers.BatchNormalization())\n",
    "m10_t1.add(layers.LeakyReLU(alpha=0.1))\n",
    "m10_t1.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "m10_t1.add(layers.Conv2D(64,(3,3), padding='same', use_bias=False))\n",
    "m10_t1.add(layers.BatchNormalization())\n",
    "m10_t1.add(layers.LeakyReLU(alpha=0.1))\n",
    "m10_t1.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "m10_t1.add(layers.Conv2D(128,(3,3), padding='same', use_bias=False))\n",
    "m10_t1.add(layers.BatchNormalization())\n",
    "m10_t1.add(layers.LeakyReLU(alpha=0.1))\n",
    "m10_t1.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "m10_t1.add(layers.Conv2D(256,(3,3), padding='same', use_bias=False))\n",
    "m10_t1.add(layers.BatchNormalization())\n",
    "m10_t1.add(layers.LeakyReLU(alpha=0.1))\n",
    "m10_t1.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "m10_t1.add(layers.Conv2D(512,(3,3), padding='same', use_bias=False))\n",
    "m10_t1.add(layers.BatchNormalization())\n",
    "m10_t1.add(layers.LeakyReLU(alpha=0.1))\n",
    "m10_t1.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "# fully connected layers\n",
    "m10_t1.add(layers.Flatten())\n",
    "m10_t1.add(layers.Dense(1024))\n",
    "m10_t1.add(layers.LeakyReLU(alpha=0.1))\n",
    "m10_t1.add(layers.Dense(n_classes, activation='softmax'))\n",
    "\n",
    "# reviewing the model architecture and adding model and name to list\n",
    "m10_t1.summary()\n",
    "model_names.append('m10_t1')\n",
    "model_list.append(m10_t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial 2 - Batch normalization after activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 200, 200, 32)      864       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 200, 200, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 200, 200, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 100, 100, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 100, 100, 64)      18432     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 100, 100, 64)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 100, 100, 64)      256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 50, 50, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 50, 50, 128)       73728     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 50, 50, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 50, 50, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 25, 25, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 25, 25, 256)       294912    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 25, 25, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 25, 25, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 12, 12, 512)       1179648   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 12, 12, 512)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 12, 12, 512)       2048      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 18432)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              18875392  \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 20)                20500     \n",
      "=================================================================\n",
      "Total params: 20,467,444\n",
      "Trainable params: 20,465,460\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# defining the model architecture\n",
    "m10_t2 = models.Sequential()\n",
    "\n",
    "# convolution/max pool stacks\n",
    "m10_t2.add(layers.Conv2D(32,(3,3), input_shape=img_shape, padding='same', use_bias=False))\n",
    "m10_t2.add(layers.LeakyReLU(alpha=0.1))\n",
    "m10_t2.add(layers.BatchNormalization())\n",
    "m10_t2.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "m10_t2.add(layers.Conv2D(64,(3,3), padding='same', use_bias=False))\n",
    "m10_t2.add(layers.LeakyReLU(alpha=0.1))\n",
    "m10_t2.add(layers.BatchNormalization())\n",
    "m10_t2.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "m10_t2.add(layers.Conv2D(128,(3,3), padding='same', use_bias=False))\n",
    "m10_t2.add(layers.LeakyReLU(alpha=0.1))\n",
    "m10_t2.add(layers.BatchNormalization())\n",
    "m10_t2.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "m10_t2.add(layers.Conv2D(256,(3,3), padding='same', use_bias=False))\n",
    "m10_t2.add(layers.LeakyReLU(alpha=0.1))\n",
    "m10_t2.add(layers.BatchNormalization())\n",
    "m10_t2.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "m10_t2.add(layers.Conv2D(512,(3,3), padding='same', use_bias=False))\n",
    "m10_t2.add(layers.LeakyReLU(alpha=0.1))\n",
    "m10_t2.add(layers.BatchNormalization())\n",
    "m10_t2.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "# fully connected layers\n",
    "m10_t2.add(layers.Flatten())\n",
    "m10_t2.add(layers.Dense(1024))\n",
    "m10_t2.add(layers.LeakyReLU(alpha=0.1))\n",
    "m10_t2.add(layers.Dense(n_classes, activation='softmax'))\n",
    "\n",
    "# reviewing the model architecture and adding model and name to list\n",
    "m10_t2.summary()\n",
    "model_names.append('m10_t2')\n",
    "model_list.append(m10_t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up standardization and augmentation parameters\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   rotation_range=40,\n",
    "                                   width_shift_range=0.3,\n",
    "                                   height_shift_range=0.3,\n",
    "                                   shear_range=0.3,\n",
    "                                   zoom_range=0.3,\n",
    "                                   fill_mode='nearest',\n",
    "                                   horizontal_flip=True,\n",
    "                                   vertical_flip=True)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 367 ms, sys: 929 ms, total: 1.3 s\n",
      "Wall time: 1.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# data standardization and augmentation\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=batch_size)\n",
    "val_generator = val_datagen.flow(X_val, y_val, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up optimizer\n",
    "opt = optimizers.Adamax(learning_rate=9e-4)\n",
    "\n",
    "# compiling loss functions\n",
    "m10_t1.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['acc'])\n",
    "m10_t2.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1013/1013 [==============================] - 104s 103ms/step - loss: 3.3765 - acc: 0.1405 - val_loss: 2.0554 - val_acc: 0.2129\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.21287, saving model to /floyd/home/m10_t1.h5\n",
      "Epoch 2/30\n",
      "1013/1013 [==============================] - 102s 101ms/step - loss: 2.6507 - acc: 0.2005 - val_loss: 3.6839 - val_acc: 0.2294\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.21287 to 0.22943, saving model to /floyd/home/m10_t1.h5\n",
      "Epoch 3/30\n",
      "1013/1013 [==============================] - 102s 101ms/step - loss: 2.4862 - acc: 0.2389 - val_loss: 2.4008 - val_acc: 0.3030\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.22943 to 0.30299, saving model to /floyd/home/m10_t1.h5\n",
      "Epoch 4/30\n",
      "1013/1013 [==============================] - 102s 100ms/step - loss: 2.3568 - acc: 0.2642 - val_loss: 2.4754 - val_acc: 0.3441\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.30299 to 0.34414, saving model to /floyd/home/m10_t1.h5\n",
      "Epoch 5/30\n",
      "1013/1013 [==============================] - 102s 101ms/step - loss: 2.2231 - acc: 0.2994 - val_loss: 2.7127 - val_acc: 0.3267\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.34414\n",
      "Epoch 6/30\n",
      "1013/1013 [==============================] - 102s 101ms/step - loss: 2.1562 - acc: 0.3200 - val_loss: 1.6467 - val_acc: 0.3628\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.34414 to 0.36284, saving model to /floyd/home/m10_t1.h5\n",
      "Epoch 7/30\n",
      "1013/1013 [==============================] - 102s 101ms/step - loss: 2.0891 - acc: 0.3384 - val_loss: 1.7297 - val_acc: 0.4302\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.36284 to 0.43017, saving model to /floyd/home/m10_t1.h5\n",
      "Epoch 8/30\n",
      "1013/1013 [==============================] - 103s 102ms/step - loss: 2.0360 - acc: 0.3528 - val_loss: 3.4097 - val_acc: 0.3566\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.43017\n",
      "Epoch 9/30\n",
      "1013/1013 [==============================] - 102s 101ms/step - loss: 2.0053 - acc: 0.3663 - val_loss: 1.5580 - val_acc: 0.4414\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.43017 to 0.44140, saving model to /floyd/home/m10_t1.h5\n",
      "Epoch 10/30\n",
      "1013/1013 [==============================] - 102s 101ms/step - loss: 1.9234 - acc: 0.3886 - val_loss: 1.2521 - val_acc: 0.4551\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.44140 to 0.45511, saving model to /floyd/home/m10_t1.h5\n",
      "Epoch 11/30\n",
      "1013/1013 [==============================] - 103s 101ms/step - loss: 1.8898 - acc: 0.3986 - val_loss: 1.6852 - val_acc: 0.4713\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.45511 to 0.47132, saving model to /floyd/home/m10_t1.h5\n",
      "Epoch 12/30\n",
      "1013/1013 [==============================] - 102s 101ms/step - loss: 1.8373 - acc: 0.4233 - val_loss: 1.6174 - val_acc: 0.4638\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.47132\n",
      "Epoch 13/30\n",
      "1013/1013 [==============================] - 101s 100ms/step - loss: 1.7902 - acc: 0.4322 - val_loss: 2.6003 - val_acc: 0.4489\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.47132\n",
      "Epoch 14/30\n",
      "1013/1013 [==============================] - 102s 101ms/step - loss: 1.7605 - acc: 0.4377 - val_loss: 1.6849 - val_acc: 0.4738\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.47132 to 0.47382, saving model to /floyd/home/m10_t1.h5\n",
      "Epoch 15/30\n",
      "1013/1013 [==============================] - 103s 101ms/step - loss: 1.7226 - acc: 0.4527 - val_loss: 1.5219 - val_acc: 0.5237\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.47382 to 0.52369, saving model to /floyd/home/m10_t1.h5\n",
      "Epoch 16/30\n",
      "1013/1013 [==============================] - 102s 101ms/step - loss: 1.6783 - acc: 0.4669 - val_loss: 0.9538 - val_acc: 0.5387\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.52369 to 0.53865, saving model to /floyd/home/m10_t1.h5\n",
      "Epoch 17/30\n",
      "1013/1013 [==============================] - 103s 102ms/step - loss: 1.6622 - acc: 0.4772 - val_loss: 2.1237 - val_acc: 0.5112\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.53865\n",
      "Epoch 18/30\n",
      "1013/1013 [==============================] - 103s 101ms/step - loss: 1.6189 - acc: 0.4848 - val_loss: 2.0315 - val_acc: 0.5187\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.53865\n",
      "Epoch 19/30\n",
      "1013/1013 [==============================] - 103s 101ms/step - loss: 1.5925 - acc: 0.4940 - val_loss: 4.3287 - val_acc: 0.3404\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.53865\n",
      "Epoch 20/30\n",
      "1013/1013 [==============================] - 103s 101ms/step - loss: 1.5685 - acc: 0.4978 - val_loss: 2.0894 - val_acc: 0.5025\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.53865\n",
      "Epoch 21/30\n",
      "1013/1013 [==============================] - 102s 101ms/step - loss: 1.5555 - acc: 0.5058 - val_loss: 1.4247 - val_acc: 0.5561\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.53865 to 0.55611, saving model to /floyd/home/m10_t1.h5\n",
      "Epoch 22/30\n",
      "1013/1013 [==============================] - 102s 101ms/step - loss: 1.5178 - acc: 0.5158 - val_loss: 1.2070 - val_acc: 0.5374\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.55611\n",
      "Epoch 23/30\n",
      "1013/1013 [==============================] - 102s 101ms/step - loss: 1.4838 - acc: 0.5226 - val_loss: 1.7219 - val_acc: 0.5948\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.55611 to 0.59476, saving model to /floyd/home/m10_t1.h5\n",
      "Epoch 24/30\n",
      "1013/1013 [==============================] - 102s 101ms/step - loss: 1.4412 - acc: 0.5338 - val_loss: 1.0093 - val_acc: 0.5711\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.59476\n",
      "Epoch 25/30\n",
      "1013/1013 [==============================] - 103s 101ms/step - loss: 1.4500 - acc: 0.5378 - val_loss: 0.5529 - val_acc: 0.4638\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.59476\n",
      "Epoch 26/30\n",
      "1013/1013 [==============================] - 102s 101ms/step - loss: 1.4342 - acc: 0.5451 - val_loss: 0.4321 - val_acc: 0.5885\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.59476\n",
      "Epoch 27/30\n",
      "1013/1013 [==============================] - 103s 101ms/step - loss: 1.3820 - acc: 0.5510 - val_loss: 4.3343 - val_acc: 0.4751\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.59476\n",
      "Epoch 28/30\n",
      "1013/1013 [==============================] - 102s 101ms/step - loss: 1.3450 - acc: 0.5722 - val_loss: 1.7644 - val_acc: 0.5823\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.59476\n",
      "Epoch 29/30\n",
      "1013/1013 [==============================] - 102s 101ms/step - loss: 1.3411 - acc: 0.5701 - val_loss: 0.8703 - val_acc: 0.6222\n",
      "\n",
      "Epoch 00029: val_acc improved from 0.59476 to 0.62219, saving model to /floyd/home/m10_t1.h5\n",
      "Epoch 30/30\n",
      "1013/1013 [==============================] - 103s 102ms/step - loss: 1.3051 - acc: 0.5823 - val_loss: 1.5013 - val_acc: 0.6172\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.62219\n",
      "CPU times: user 1h 7min 31s, sys: 12min 10s, total: 1h 19min 41s\n",
      "Wall time: 51min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# setting up model saving checkpoints\n",
    "m10_t1_cp = ModelCheckpoint(filepath=f'{model_path}m10_t1.h5',\n",
    "                              monitor='val_acc',\n",
    "                              verbose=1,\n",
    "                              save_best_only=True)\n",
    "\n",
    "# fitting model\n",
    "m10_t1_history = m10_t1.fit(train_generator,\n",
    "                                steps_per_epoch=len(X_train)//batch_size,\n",
    "                                epochs=n_epochs,\n",
    "                                callbacks=[m10_t1_cp],\n",
    "                                validation_data=val_generator,\n",
    "                                validation_steps=len(X_val)//batch_size)\n",
    "\n",
    "# adding training history to list\n",
    "model_hists.append(m10_t1_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1013/1013 [==============================] - 89s 88ms/step - loss: 3.2214 - acc: 0.1622 - val_loss: 3.2968 - val_acc: 0.2562\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.25619, saving model to /floyd/home/m10_t2.h5\n",
      "Epoch 2/30\n",
      "1013/1013 [==============================] - 88s 87ms/step - loss: 2.9494 - acc: 0.2081 - val_loss: 3.0921 - val_acc: 0.2818\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.25619 to 0.28180, saving model to /floyd/home/m10_t2.h5\n",
      "Epoch 3/30\n",
      "1013/1013 [==============================] - 88s 87ms/step - loss: 2.7682 - acc: 0.2354 - val_loss: 2.2894 - val_acc: 0.3815\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.28180 to 0.38155, saving model to /floyd/home/m10_t2.h5\n",
      "Epoch 4/30\n",
      "1013/1013 [==============================] - 88s 87ms/step - loss: 2.4854 - acc: 0.2838 - val_loss: 3.5647 - val_acc: 0.3379\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.38155\n",
      "Epoch 5/30\n",
      "1013/1013 [==============================] - 88s 87ms/step - loss: 2.2888 - acc: 0.3159 - val_loss: 4.0560 - val_acc: 0.2781\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.38155\n",
      "Epoch 6/30\n",
      "1013/1013 [==============================] - 88s 87ms/step - loss: 2.1521 - acc: 0.3532 - val_loss: 2.2990 - val_acc: 0.3541\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.38155\n",
      "Epoch 7/30\n",
      "1013/1013 [==============================] - 89s 88ms/step - loss: 2.0534 - acc: 0.3698 - val_loss: 2.3862 - val_acc: 0.4738\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.38155 to 0.47382, saving model to /floyd/home/m10_t2.h5\n",
      "Epoch 8/30\n",
      "1013/1013 [==============================] - 88s 87ms/step - loss: 1.9507 - acc: 0.3964 - val_loss: 2.9843 - val_acc: 0.5224\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.47382 to 0.52244, saving model to /floyd/home/m10_t2.h5\n",
      "Epoch 9/30\n",
      "1013/1013 [==============================] - 87s 86ms/step - loss: 1.8732 - acc: 0.4196 - val_loss: 1.6633 - val_acc: 0.4289\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.52244\n",
      "Epoch 10/30\n",
      "1013/1013 [==============================] - 88s 87ms/step - loss: 1.7882 - acc: 0.4406 - val_loss: 0.9275 - val_acc: 0.4726\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.52244\n",
      "Epoch 11/30\n",
      "1013/1013 [==============================] - 88s 87ms/step - loss: 1.7448 - acc: 0.4500 - val_loss: 1.5891 - val_acc: 0.5436\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.52244 to 0.54364, saving model to /floyd/home/m10_t2.h5\n",
      "Epoch 12/30\n",
      "1013/1013 [==============================] - 88s 87ms/step - loss: 1.6936 - acc: 0.4721 - val_loss: 2.6218 - val_acc: 0.5461\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.54364 to 0.54613, saving model to /floyd/home/m10_t2.h5\n",
      "Epoch 13/30\n",
      "1013/1013 [==============================] - 88s 87ms/step - loss: 1.6371 - acc: 0.4873 - val_loss: 3.9654 - val_acc: 0.5100\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.54613\n",
      "Epoch 14/30\n",
      "1013/1013 [==============================] - 88s 86ms/step - loss: 1.6014 - acc: 0.5052 - val_loss: 4.0207 - val_acc: 0.5050\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.54613\n",
      "Epoch 15/30\n",
      "1013/1013 [==============================] - 88s 87ms/step - loss: 1.5466 - acc: 0.5167 - val_loss: 1.6899 - val_acc: 0.5998\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.54613 to 0.59975, saving model to /floyd/home/m10_t2.h5\n",
      "Epoch 16/30\n",
      "1013/1013 [==============================] - 88s 87ms/step - loss: 1.5115 - acc: 0.5222 - val_loss: 1.6234 - val_acc: 0.5299\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.59975\n",
      "Epoch 17/30\n",
      "1013/1013 [==============================] - 88s 87ms/step - loss: 1.4508 - acc: 0.5437 - val_loss: 1.4824 - val_acc: 0.5748\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.59975\n",
      "Epoch 18/30\n",
      "1013/1013 [==============================] - 88s 87ms/step - loss: 1.4192 - acc: 0.5484 - val_loss: 1.2774 - val_acc: 0.5773\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.59975\n",
      "Epoch 19/30\n",
      "1013/1013 [==============================] - 88s 87ms/step - loss: 1.3981 - acc: 0.5596 - val_loss: 2.1045 - val_acc: 0.5698\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.59975\n",
      "Epoch 20/30\n",
      "1013/1013 [==============================] - 88s 87ms/step - loss: 1.3541 - acc: 0.5728 - val_loss: 1.6557 - val_acc: 0.5461\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.59975\n",
      "Epoch 21/30\n",
      "1013/1013 [==============================] - 88s 87ms/step - loss: 1.3280 - acc: 0.5799 - val_loss: 1.6330 - val_acc: 0.5910\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.59975\n",
      "Epoch 22/30\n",
      "1013/1013 [==============================] - 88s 87ms/step - loss: 1.3113 - acc: 0.5791 - val_loss: 1.9717 - val_acc: 0.5586\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.59975\n",
      "Epoch 23/30\n",
      "1013/1013 [==============================] - 88s 87ms/step - loss: 1.2726 - acc: 0.5922 - val_loss: 3.1709 - val_acc: 0.5549\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.59975\n",
      "Epoch 24/30\n",
      "1013/1013 [==============================] - 87s 86ms/step - loss: 1.2528 - acc: 0.6040 - val_loss: 1.6396 - val_acc: 0.6010\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.59975 to 0.60100, saving model to /floyd/home/m10_t2.h5\n",
      "Epoch 25/30\n",
      "1013/1013 [==============================] - 88s 87ms/step - loss: 1.2345 - acc: 0.6114 - val_loss: 3.9625 - val_acc: 0.6060\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.60100 to 0.60599, saving model to /floyd/home/m10_t2.h5\n",
      "Epoch 26/30\n",
      "1013/1013 [==============================] - 88s 87ms/step - loss: 1.1978 - acc: 0.6177 - val_loss: 1.8170 - val_acc: 0.5312\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.60599\n",
      "Epoch 27/30\n",
      "1013/1013 [==============================] - 88s 87ms/step - loss: 1.1823 - acc: 0.6305 - val_loss: 1.3809 - val_acc: 0.5773\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.60599\n",
      "Epoch 28/30\n",
      "1013/1013 [==============================] - 88s 87ms/step - loss: 1.1589 - acc: 0.6354 - val_loss: 1.9832 - val_acc: 0.6434\n",
      "\n",
      "Epoch 00028: val_acc improved from 0.60599 to 0.64339, saving model to /floyd/home/m10_t2.h5\n",
      "Epoch 29/30\n",
      "1013/1013 [==============================] - 88s 87ms/step - loss: 1.1495 - acc: 0.6385 - val_loss: 1.9022 - val_acc: 0.6509\n",
      "\n",
      "Epoch 00029: val_acc improved from 0.64339 to 0.65087, saving model to /floyd/home/m10_t2.h5\n",
      "Epoch 30/30\n",
      "1013/1013 [==============================] - 88s 87ms/step - loss: 1.1399 - acc: 0.6322 - val_loss: 1.4360 - val_acc: 0.5885\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.65087\n",
      "CPU times: user 1h 4min 19s, sys: 10min 30s, total: 1h 14min 49s\n",
      "Wall time: 44min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# setting up model saving checkpoints\n",
    "m10_t2_cp = ModelCheckpoint(filepath=f'{model_path}m10_t2.h5',\n",
    "                              monitor='val_acc',\n",
    "                              verbose=1,\n",
    "                              save_best_only=True)\n",
    "\n",
    "# fitting model\n",
    "m10_t2_history = m10_t2.fit(train_generator,\n",
    "                                steps_per_epoch=len(X_train)//batch_size,\n",
    "                                epochs=n_epochs,\n",
    "                                callbacks=[m10_t2_cp],\n",
    "                                validation_data=val_generator,\n",
    "                                validation_steps=len(X_val)//batch_size)\n",
    "\n",
    "# adding training history to list\n",
    "model_hists.append(m10_t2_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dictionary for model names, models, and histories from respective lists\n",
    "models_dict = {i:[j,k] for i,j,k in zip(model_names,model_list,model_hists)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 3s 28ms/step\n",
      "m10_t1 Val Accuracy: 62.47%\n",
      "m10_t1 Val Loss: 0.1366\n",
      "---\n",
      "102/102 [==============================] - 3s 28ms/step\n",
      "m10_t2 Val Accuracy: 64.69%\n",
      "m10_t2 Val Loss: 0.1167\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# evaluating models on validation set\n",
    "for key, value in models_dict.items():\n",
    "    model = models.load_model(f'{model_path}{key}.h5')\n",
    "    (val_loss, val_accuracy) = model.evaluate(val_generator,verbose=1)\n",
    "    print(f'{key} Val Accuracy: {round((val_accuracy*100),2)}%')\n",
    "    print(f'{key} Val Loss: {round(val_loss,4)}')\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m10_t1_history saved in /floyd/home/\n",
      "m10_t2_history saved in /floyd/home/\n"
     ]
    }
   ],
   "source": [
    "# saving training histories\n",
    "for key, value in models_dict.items():\n",
    "    with open(f'{model_path}{key}_history', 'wb') as file_pi:\n",
    "        pickle.dump(value[1].history, file_pi)\n",
    "    print(f'{key}_history saved in {model_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
