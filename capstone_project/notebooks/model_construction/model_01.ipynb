{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the construction and training of the baseline model. The notebook is split up into four sections: training mode selection (where the model will run), set-up, model constrution, and training. \n",
    "\n",
    "Evaluation will take place in the *model_optimization_and_evaluation.ipynb* notebook found in the *notebooks* folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# importing necessary packages and libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras import applications \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Training Mode Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, specify the training mode for the model. This will determine the location from which the source data is drawn, and to which the trained models (and training histories) are saved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **training_mode = 'floydhub'** (runs on Floydhub)\n",
    "- **training_mode = 'local'** (runs on local disk and processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select training mode\n",
    "training_mode = 'floydhub'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory base paths\n",
    "data_path_local = '../../data/0002_array_data/train_data/'\n",
    "model_path_local = '../../notebooks/model_construction/saved_models/'\n",
    "data_path_floydhub = '/floyd/input/capstone_mushrooms/'\n",
    "model_path_floydhub = '/floyd/home/'\n",
    "\n",
    "# setting directory paths based on training mode selection\n",
    "if training_mode == 'floydhub':\n",
    "    data_path = data_path_floydhub\n",
    "    model_path = model_path_floydhub\n",
    "    if training_mode == 'local':\n",
    "        data_path = data_path_local\n",
    "        model_path = model_path_local\n",
    "else:\n",
    "    raise Exception('Please choose valid training mode: \"floydhub\" or \"local\".')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the training and validation data subsets\n",
    "X_train = np.load(f'{data_path}X_train_data.npy')\n",
    "y_train = np.load(f'{data_path}y_train_data.npy')\n",
    "X_val = np.load(f'{data_path}X_val_data.npy')\n",
    "y_val = np.load(f'{data_path}y_val_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting training parameters\n",
    "batch_size = 32\n",
    "n_classes = 20\n",
    "n_epochs = 15\n",
    "img_shape = X_train.shape[1:]\n",
    "model_names = []\n",
    "model_list = []\n",
    "model_hists = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Model Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial 1 - Simple Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 200, 200, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 100, 100, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 100, 100, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 50, 50, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 50, 50, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 25, 25, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 25, 25, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 36864)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               18874880  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                10260     \n",
      "=================================================================\n",
      "Total params: 19,273,556\n",
      "Trainable params: 19,273,556\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# defining the model architecture\n",
    "model_01 = models.Sequential()\n",
    "\n",
    "# convolution/max pool stacks\n",
    "model_01.add(layers.Conv2D(32,(3,3), activation='relu', input_shape=img_shape, padding='same'))\n",
    "model_01.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "model_01.add(layers.Conv2D(64,(3,3), activation='relu', padding='same'))\n",
    "model_01.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "model_01.add(layers.Conv2D(128,(3,3), activation='relu', padding='same'))\n",
    "model_01.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "model_01.add(layers.Conv2D(256,(3,3), activation='relu', padding='same'))\n",
    "model_01.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "# fully connected layers\n",
    "model_01.add(layers.Flatten())\n",
    "model_01.add(layers.Dense(512, activation='relu'))\n",
    "model_01.add(layers.Dense(n_classes, activation='softmax'))\n",
    "\n",
    "# reviewing the model architecture and adding model and name to list\n",
    "model_01.summary()\n",
    "model_names.append('model_01')\n",
    "model_list.append(model_01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up standardization and augmentation parameters\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 387 ms, sys: 945 ms, total: 1.33 s\n",
      "Wall time: 1.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# data standardization and augmentation\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=batch_size)\n",
    "val_generator = val_datagen.flow(X_val, y_val, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling loss functions\n",
    "model_01.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "253/253 [==============================] - 97s 383ms/step - loss: 2.6000 - acc: 0.1677 - val_loss: 1.9802 - val_acc: 0.2425\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.24250, saving model to /floyd/home/model_01.h5\n",
      "Epoch 2/15\n",
      "253/253 [==============================] - 30s 120ms/step - loss: 2.1427 - acc: 0.3093 - val_loss: 1.8360 - val_acc: 0.3535\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.24250 to 0.35347, saving model to /floyd/home/model_01.h5\n",
      "Epoch 3/15\n",
      "253/253 [==============================] - 30s 120ms/step - loss: 1.8194 - acc: 0.4156 - val_loss: 1.5710 - val_acc: 0.4087\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.35347 to 0.40874, saving model to /floyd/home/model_01.h5\n",
      "Epoch 4/15\n",
      "253/253 [==============================] - 31s 121ms/step - loss: 1.5388 - acc: 0.5007 - val_loss: 2.2803 - val_acc: 0.4190\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.40874 to 0.41902, saving model to /floyd/home/model_01.h5\n",
      "Epoch 5/15\n",
      "253/253 [==============================] - 30s 120ms/step - loss: 1.1840 - acc: 0.6211 - val_loss: 1.7443 - val_acc: 0.4486\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.41902 to 0.44859, saving model to /floyd/home/model_01.h5\n",
      "Epoch 6/15\n",
      "253/253 [==============================] - 30s 120ms/step - loss: 0.7905 - acc: 0.7417 - val_loss: 2.1930 - val_acc: 0.4486\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.44859\n",
      "Epoch 7/15\n",
      "253/253 [==============================] - 31s 121ms/step - loss: 0.4744 - acc: 0.8493 - val_loss: 2.1542 - val_acc: 0.4730\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.44859 to 0.47301, saving model to /floyd/home/model_01.h5\n",
      "Epoch 8/15\n",
      "253/253 [==============================] - 30s 120ms/step - loss: 0.2752 - acc: 0.9134 - val_loss: 3.1427 - val_acc: 0.4344\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.47301\n",
      "Epoch 9/15\n",
      "253/253 [==============================] - 31s 121ms/step - loss: 0.1569 - acc: 0.9499 - val_loss: 3.4535 - val_acc: 0.4036\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.47301\n",
      "Epoch 10/15\n",
      "253/253 [==============================] - 30s 121ms/step - loss: 0.1223 - acc: 0.9661 - val_loss: 3.8275 - val_acc: 0.4190\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.47301\n",
      "Epoch 11/15\n",
      "253/253 [==============================] - 31s 121ms/step - loss: 0.1063 - acc: 0.9715 - val_loss: 3.7258 - val_acc: 0.4447\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.47301\n",
      "Epoch 12/15\n",
      "253/253 [==============================] - 31s 122ms/step - loss: 0.0675 - acc: 0.9838 - val_loss: 4.8093 - val_acc: 0.4267\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.47301\n",
      "Epoch 13/15\n",
      "253/253 [==============================] - 31s 121ms/step - loss: 0.0724 - acc: 0.9807 - val_loss: 5.5578 - val_acc: 0.3985\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.47301\n",
      "Epoch 14/15\n",
      "253/253 [==============================] - 31s 122ms/step - loss: 0.0895 - acc: 0.9747 - val_loss: 5.3194 - val_acc: 0.4036\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.47301\n",
      "Epoch 15/15\n",
      "253/253 [==============================] - 31s 121ms/step - loss: 0.0773 - acc: 0.9792 - val_loss: 4.0738 - val_acc: 0.4229\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.47301\n",
      "CPU times: user 4min 41s, sys: 1min 52s, total: 6min 33s\n",
      "Wall time: 8min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# setting up model saving checkpoints\n",
    "model_01_cp = ModelCheckpoint(filepath=f'{model_path}model_01.h5',\n",
    "                              monitor='val_acc',\n",
    "                              verbose=1,\n",
    "                              save_best_only=True)\n",
    "\n",
    "# fitting model\n",
    "model_01_history = model_01.fit(train_generator,\n",
    "                                steps_per_epoch=len(X_train)//batch_size,\n",
    "                                epochs=n_epochs,\n",
    "                                callbacks=[model_01_cp],\n",
    "                                validation_data=val_generator,\n",
    "                                validation_steps=len(X_val)//batch_size)\n",
    "\n",
    "# adding training history to list\n",
    "model_hists.append(model_01_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dictionary for model names, models, and histories from respective lists\n",
    "models_dict = {i:[j,k] for i,j,k in zip(model_names,model_list,model_hists)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 1s 42ms/step\n",
      "model_01 Val Accuracy: 42.35%\n",
      "model_01 Val Loss: 4.7797\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# evaluating models on validation set\n",
    "for key, value in models_dict.items():\n",
    "    (val_loss, val_accuracy) = value[0].evaluate(val_generator,verbose=1)\n",
    "    print(f'{key} Val Accuracy: {round((val_accuracy*100),2)}%')\n",
    "    print(f'{key} Val Loss: {round(val_loss,4)}')\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_01_history saved in /floyd/home/\n"
     ]
    }
   ],
   "source": [
    "# saving training histories\n",
    "for key, value in models_dict.items():\n",
    "    with open(f'{model_path}{key}_history', 'wb') as file_pi:\n",
    "        pickle.dump(value[1].history, file_pi)\n",
    "    print(f'{key}_history saved in {model_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
